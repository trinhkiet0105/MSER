{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "lib_path = os.path.abspath(\"\").replace(\"notebooks\", \"src\")\n",
    "sys.path.append(lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MMSERA\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import BertTokenizer, RobertaTokenizer,AutoTokenizer\n",
    "\n",
    "from configs.base import Config\n",
    "from data.dataloader import build_train_test_dataset\n",
    "from models import losses, networks\n",
    "from trainer import Trainer\n",
    "from utils.configs import get_options\n",
    "from utils.torch.callbacks import CheckpointsCallback\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:58:06,906 - root - INFO - Initializing model...\n"
     ]
    }
   ],
   "source": [
    "opt = get_options(f\"{lib_path}/configs/bert_vggish.py\")\n",
    "logging.info(\"Initializing model...\")\n",
    "# Model\n",
    "try:\n",
    "    network = getattr(networks, opt.model_type)(\n",
    "        num_classes=opt.num_classes,\n",
    "        num_attention_head=opt.num_attention_head,\n",
    "        dropout=opt.dropout,\n",
    "        text_encoder_type=opt.text_encoder_type,\n",
    "        text_encoder_dim=opt.text_encoder_dim,\n",
    "        text_unfreeze=opt.text_unfreeze,\n",
    "        audio_encoder_type=opt.audio_encoder_type,\n",
    "        audio_encoder_dim=opt.audio_encoder_dim,\n",
    "        audio_unfreeze=opt.audio_unfreeze,\n",
    "        audio_norm_type=opt.audio_norm_type,\n",
    "        fusion_head_output_type=opt.fusion_head_output_type,\n",
    "    )\n",
    "    network.to(device)\n",
    "except AttributeError:\n",
    "    raise NotImplementedError(\"Model {} is not implemented\".format(opt.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMSERA(\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): Wav2Vec2Base(\n",
       "    (model): Wav2Vec2Model(\n",
       "      (feature_extractor): FeatureExtractor(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayerBlock(\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          )\n",
       "          (1-4): 4 x ConvLayerBlock(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          )\n",
       "          (5-6): 2 x ConvLayerBlock(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder): Encoder(\n",
       "        (feature_projection): FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "            (conv): ParametrizedConv1d(\n",
       "              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x EncoderLayer(\n",
       "              (attention): SelfAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (feed_forward): FeedForward(\n",
       "                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (text_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (text_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (fusion_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (fusion_linear): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fusion_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (classifer): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:58:11,733 - root - INFO - Initializing checkpoint directory and dataset...\n",
      "2023-11-18 14:58:12,901 - root - INFO - \n",
      "             audio_encoder_dim: 768                                     \n",
      "            audio_encoder_type: wav2vec                                 \n",
      "              audio_max_length: 50                                      \n",
      "               audio_norm_type: layer_norm                              \n",
      "                audio_unfreeze: False                                   \n",
      "                    batch_size: 1                                       \n",
      "                checkpoint_dir: d:\\MMSERA\\notebooks\\checkpoints\\bert_vggish_MMSERA\\20231118-145812\n",
      "                     data_root: D:/bert-based-selfalign/IEMOCAP_wav2vec/IEMOCAP_4_class\n",
      "                       dropout: 0.5                                     \n",
      "                      feat_dim: 2048                                    \n",
      "              focal_loss_alpha: None                                    \n",
      "              focal_loss_gamma: 0.5                                     \n",
      "       fusion_head_output_type: mean                                    \n",
      "                      lambda_c: 1.0                                     \n",
      "                 learning_rate: 0.0001                                  \n",
      "           learning_rate_gamma: 0.1                                     \n",
      "       learning_rate_step_size: 30                                      \n",
      "                     loss_type: CrossEntropyLoss                        \n",
      "                margin_loss_m1: 1.0                                     \n",
      "                margin_loss_m2: 0.5                                     \n",
      "                margin_loss_m3: 0.0                                     \n",
      "             margin_loss_scale: 64.0                                    \n",
      "                   max_to_keep: 1                                       \n",
      "                    model_type: MMSERA                                  \n",
      "                          name: bert_vggish_MMSERA                      \n",
      "            num_attention_head: 8                                       \n",
      "                   num_classes: 4                                       \n",
      "                    num_epochs: 250                                     \n",
      "              optim_attributes: None                                    \n",
      "                        resume: False                                   \n",
      "                   resume_path: D:/SER_ICIIT_2024/notebooks/checkpoints/bert_vggish_MMSERA/20230928-020711/weights/checkpoint_35_132000.pt\n",
      "               save_all_states: True                                    \n",
      "                 save_best_val: True                                    \n",
      "                     save_freq: 4000                                    \n",
      "              text_encoder_dim: 768                                     \n",
      "             text_encoder_type: bert                                    \n",
      "               text_max_length: 297                                     \n",
      "                 text_unfreeze: False                                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"Initializing checkpoint directory and dataset...\")\n",
    "if opt.text_encoder_type == \"bert\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "elif opt.text_encoder_type == \"roberta\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "else:\n",
    "    raise NotImplementedError(\"Tokenizer {} is not implemented\".format(opt.text_encoder_type))\n",
    "\n",
    "# Preapre the checkpoint directory\n",
    "opt.checkpoint_dir = checkpoint_dir = os.path.join(\n",
    "    os.path.abspath(opt.checkpoint_dir),\n",
    "    opt.name,\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "log_dir = os.path.join(checkpoint_dir, \"logs\")\n",
    "weight_dir = os.path.join(checkpoint_dir, \"weights\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(weight_dir, exist_ok=True)\n",
    "opt.save(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "train_ds, test_ds = build_train_test_dataset(\n",
    "    opt.data_root,\n",
    "    opt.batch_size,\n",
    "    tokenizer,\n",
    "    opt.audio_max_length,\n",
    "    text_max_length=opt.text_max_length,\n",
    "    audio_encoder_type=opt.audio_encoder_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:55:45,309 - root - INFO - Initializing trainer...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Initializing trainer...\")\n",
    "if opt.loss_type == \"FocalLoss\":\n",
    "    criterion = losses.FocalLoss(gamma=opt.focal_loss_gamma, alpha=opt.focal_loss_alpha)\n",
    "    criterion.to(device)\n",
    "else:\n",
    "    try:\n",
    "        criterion = getattr(losses, opt.loss_type)(\n",
    "            feat_dim=opt.feat_dim,\n",
    "            num_classes=opt.num_classes,\n",
    "            lambda_c=opt.lambda_c,\n",
    "        )\n",
    "        criterion.to(device)\n",
    "    except AttributeError:\n",
    "        raise NotImplementedError(\"Loss {} is not implemented\".format(opt.loss_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    network=network,\n",
    "    criterion=criterion,\n",
    "    log_dir=opt.checkpoint_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-18 14:55:45,356 - root - INFO - Start training...\n",
      "2023-11-18 14:55:46,592 - root - WARNING - When save_best_val is True, please make sure that you pass the validation data to the trainer.fit() method.\n",
      "                            Otherwise, the best model will not be saved.\n",
      "                            The model will save the lowest validation value if the metric starts with 'loss' and the highest value otherwise.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start training...\")\n",
    "# Build optimizer and criterion\n",
    "optimizer = optim.Adam(params=trainer.network.parameters(), lr=opt.learning_rate)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.learning_rate_step_size, gamma=opt.learning_rate_gamma)\n",
    "\n",
    "ckpt_callback = CheckpointsCallback(\n",
    "    checkpoint_dir=weight_dir,\n",
    "    save_freq=opt.save_freq,\n",
    "    max_to_keep=opt.max_to_keep,\n",
    "    save_best_val=opt.save_best_val,\n",
    "    save_all_states=opt.save_all_states,\n",
    ")\n",
    "trainer.compile(optimizer=optimizer, scheduler=lr_scheduler)\n",
    "if opt.resume:\n",
    "    trainer.load_all_states(opt.resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/250\n",
      "2023-11-18 14:55:47,484 - Training - INFO - Epoch 0/250\n",
      "  0%|          | 1/3230 [00:00<14:58,  3.59it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\MMSERA\\notebooks\\train.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MMSERA/notebooks/train.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_ds, opt\u001b[39m.\u001b[39;49mnum_epochs, test_ds, callbacks\u001b[39m=\u001b[39;49m[ckpt_callback])\n",
      "File \u001b[1;32md:\\MMSERA\\src\\utils\\torch\\trainer.py:327\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[1;34m(self, train_data, epochs, eval_data, test_data, callbacks)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    326\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m     global_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(global_step, epoch, train_data, eval_data, logger, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler(global_step, epoch)\n\u001b[0;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m test_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MMSERA\\src\\utils\\torch\\trainer.py:69\u001b[0m, in \u001b[0;36mTorchTrainer.train_epoch\u001b[1;34m(self, step, epoch, train_data, eval_data, logger, callbacks)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mwith\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_data), ascii\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m     68\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m     \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m train_data:\n\u001b[0;32m     70\u001b[0m         \u001b[39m# Training step\u001b[39;49;00m\n\u001b[0;32m     71\u001b[0m         step \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m\n\u001b[0;32m     72\u001b[0m         train_log \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(batch)\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:150\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m             \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    148\u001b[0m             \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n\u001b[1;32m--> 150\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_ds, opt.num_epochs, test_ds, callbacks=[ckpt_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3m-ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
