{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "lib_path = os.path.abspath(\"\").replace(\"notebooks\", \"src\")\n",
    "sys.path.append(lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SER_ICIIT_2024\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import BertTokenizer, RobertaTokenizer,AutoTokenizer\n",
    "\n",
    "from configs.base import Config\n",
    "from data.dataloader import build_train_test_dataset\n",
    "from models import losses, networks\n",
    "from trainer import Trainer\n",
    "from utils.configs import get_options\n",
    "from utils.torch.callbacks import CheckpointsCallback\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 10:03:49,803 - root - INFO - Initializing model...\n"
     ]
    }
   ],
   "source": [
    "opt = get_options(f\"{lib_path}/configs/bert_vggish.py\")\n",
    "logging.info(\"Initializing model...\")\n",
    "# Model\n",
    "try:\n",
    "    network = getattr(networks, opt.model_type)(\n",
    "        num_classes=opt.num_classes,\n",
    "        num_attention_head=opt.num_attention_head,\n",
    "        dropout=opt.dropout,\n",
    "        text_encoder_type=opt.text_encoder_type,\n",
    "        text_encoder_dim=opt.text_encoder_dim,\n",
    "        text_unfreeze=opt.text_unfreeze,\n",
    "        audio_encoder_type=opt.audio_encoder_type,\n",
    "        audio_encoder_dim=opt.audio_encoder_dim,\n",
    "        audio_unfreeze=opt.audio_unfreeze,\n",
    "        audio_norm_type=opt.audio_norm_type,\n",
    "        fusion_head_output_type=opt.fusion_head_output_type,\n",
    "    )\n",
    "    network.to(device)\n",
    "except AttributeError:\n",
    "    raise NotImplementedError(\"Model {} is not implemented\".format(opt.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMSERA(\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): VGGish(\n",
       "    (vggish): VGG(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "        (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (9): ReLU(inplace=True)\n",
       "        (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (12): ReLU(inplace=True)\n",
       "        (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (14): ReLU(inplace=True)\n",
       "        (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (embeddings): Sequential(\n",
       "        (0): Linear(in_features=12288, out_features=4096, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Linear(in_features=4096, out_features=128, bias=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (text_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_linear): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (text_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fusion_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (fusion_linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fusion_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (classifer): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 10:03:54,194 - root - INFO - Initializing checkpoint directory and dataset...\n",
      "2023-10-11 10:03:54,689 - root - INFO - \n",
      "             audio_encoder_dim: 128                                     \n",
      "            audio_encoder_type: vggish                                  \n",
      "              audio_max_length: 50                                      \n",
      "               audio_norm_type: layer_norm                              \n",
      "                audio_unfreeze: False                                   \n",
      "                    batch_size: 4                                       \n",
      "                checkpoint_dir: d:\\SER_ICIIT_2024\\notebooks\\checkpoints\\bert_vggish_MMSERA\\20231011-100354\n",
      "                     data_root: D:/MELD/MELD                            \n",
      "                       dropout: 0.5                                     \n",
      "                      feat_dim: 2048                                    \n",
      "              focal_loss_alpha: None                                    \n",
      "              focal_loss_gamma: 0.5                                     \n",
      "       fusion_head_output_type: mean                                    \n",
      "                      lambda_c: 1.0                                     \n",
      "                 learning_rate: 0.0001                                  \n",
      "           learning_rate_gamma: 0.1                                     \n",
      "       learning_rate_step_size: 30                                      \n",
      "                     loss_type: CrossEntropyLoss                        \n",
      "                margin_loss_m1: 1.0                                     \n",
      "                margin_loss_m2: 0.5                                     \n",
      "                margin_loss_m3: 0.0                                     \n",
      "             margin_loss_scale: 64.0                                    \n",
      "                   max_to_keep: 1                                       \n",
      "                    model_type: MMSERA                                  \n",
      "                          name: bert_vggish_MMSERA                      \n",
      "            num_attention_head: 8                                       \n",
      "                   num_classes: 4                                       \n",
      "                    num_epochs: 250                                     \n",
      "              optim_attributes: None                                    \n",
      "                        resume: False                                   \n",
      "                   resume_path: D:/SER_ICIIT_2024/notebooks/checkpoints/bert_vggish_MMSERA/20230928-020711/weights/checkpoint_35_132000.pt\n",
      "               save_all_states: True                                    \n",
      "                 save_best_val: True                                    \n",
      "                     save_freq: 4000                                    \n",
      "              text_encoder_dim: 768                                     \n",
      "             text_encoder_type: bert                                    \n",
      "               text_max_length: 297                                     \n",
      "                 text_unfreeze: False                                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"Initializing checkpoint directory and dataset...\")\n",
    "if opt.text_encoder_type == \"bert\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "elif opt.text_encoder_type == \"roberta\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "else:\n",
    "    raise NotImplementedError(\"Tokenizer {} is not implemented\".format(opt.text_encoder_type))\n",
    "\n",
    "# Preapre the checkpoint directory\n",
    "opt.checkpoint_dir = checkpoint_dir = os.path.join(\n",
    "    os.path.abspath(opt.checkpoint_dir),\n",
    "    opt.name,\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "log_dir = os.path.join(checkpoint_dir, \"logs\")\n",
    "weight_dir = os.path.join(checkpoint_dir, \"weights\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(weight_dir, exist_ok=True)\n",
    "opt.save(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "train_ds, test_ds = build_train_test_dataset(\n",
    "    opt.data_root,\n",
    "    opt.batch_size,\n",
    "    tokenizer,\n",
    "    opt.audio_max_length,\n",
    "    text_max_length=opt.text_max_length,\n",
    "    audio_encoder_type=opt.audio_encoder_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 10:03:56,349 - root - INFO - Initializing trainer...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Initializing trainer...\")\n",
    "if opt.loss_type == \"FocalLoss\":\n",
    "    criterion = losses.FocalLoss(gamma=opt.focal_loss_gamma, alpha=opt.focal_loss_alpha)\n",
    "    criterion.to(device)\n",
    "else:\n",
    "    try:\n",
    "        criterion = getattr(losses, opt.loss_type)(\n",
    "            feat_dim=opt.feat_dim,\n",
    "            num_classes=opt.num_classes,\n",
    "            lambda_c=opt.lambda_c,\n",
    "        )\n",
    "        criterion.to(device)\n",
    "    except AttributeError:\n",
    "        raise NotImplementedError(\"Loss {} is not implemented\".format(opt.loss_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    network=network,\n",
    "    criterion=criterion,\n",
    "    log_dir=opt.checkpoint_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 10:03:56,375 - root - INFO - Start training...\n",
      "2023-10-11 10:03:56,795 - root - WARNING - When save_best_val is True, please make sure that you pass the validation data to the trainer.fit() method.\n",
      "                            Otherwise, the best model will not be saved.\n",
      "                            The model will save the lowest validation value if the metric starts with 'loss' and the highest value otherwise.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start training...\")\n",
    "# Build optimizer and criterion\n",
    "optimizer = optim.Adam(params=trainer.network.parameters(), lr=opt.learning_rate)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.learning_rate_step_size, gamma=opt.learning_rate_gamma)\n",
    "\n",
    "ckpt_callback = CheckpointsCallback(\n",
    "    checkpoint_dir=weight_dir,\n",
    "    save_freq=opt.save_freq,\n",
    "    max_to_keep=opt.max_to_keep,\n",
    "    save_best_val=opt.save_best_val,\n",
    "    save_all_states=opt.save_all_states,\n",
    ")\n",
    "trainer.compile(optimizer=optimizer, scheduler=lr_scheduler)\n",
    "if opt.resume:\n",
    "    trainer.load_all_states(opt.resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/250\n",
      "2023-10-11 10:03:57,142 - Training - INFO - Epoch 0/250\n",
      "  0%|          | 1/1521 [00:00<00:09, 166.74it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\SER_ICIIT_2024\\notebooks\\train.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/SER_ICIIT_2024/notebooks/train.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_ds, opt\u001b[39m.\u001b[39;49mnum_epochs, test_ds, callbacks\u001b[39m=\u001b[39;49m[ckpt_callback])\n",
      "File \u001b[1;32md:\\SER_ICIIT_2024\\src\\utils\\torch\\trainer.py:327\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[1;34m(self, train_data, epochs, eval_data, test_data, callbacks)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    326\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m     global_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(global_step, epoch, train_data, eval_data, logger, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler(global_step, epoch)\n\u001b[0;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m test_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\SER_ICIIT_2024\\src\\utils\\torch\\trainer.py:72\u001b[0m, in \u001b[0;36mTorchTrainer.train_epoch\u001b[1;34m(self, step, epoch, train_data, eval_data, logger, callbacks)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_data:\n\u001b[0;32m     70\u001b[0m     \u001b[39m# Training step\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 72\u001b[0m     train_log \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(batch)\n\u001b[0;32m     73\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(train_log, \u001b[39mdict\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mtrain_step should return a dict.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m     \u001b[39m# Add logs, update progress bar\u001b[39;00m\n",
      "File \u001b[1;32md:\\SER_ICIIT_2024\\src\\trainer.py:32\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     29\u001b[0m attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m video_embedding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     video_embedding \u001b[39m=\u001b[39m video_embedding\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m     35\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(input_ids, audio, attention_mask\u001b[39m=\u001b[39mattention_mask, video_embedding\u001b[39m=\u001b[39mvideo_embedding)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "trainer.fit(train_ds, opt.num_epochs, test_ds, callbacks=[ckpt_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3m-ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
