{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "lib_path = os.path.abspath(\"\").replace(\"notebooks\", \"src\")\n",
    "sys.path.append(lib_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MMSERA\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from transformers import BertTokenizer, RobertaTokenizer,AutoTokenizer\n",
    "\n",
    "from configs.base import Config\n",
    "from data.dataloader import build_train_test_dataset\n",
    "from models import losses, networks\n",
    "from trainer import Trainer\n",
    "from utils.configs import get_options\n",
    "from utils.torch.callbacks import CheckpointsCallback\n",
    "\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:46:00,609 - root - INFO - Initializing model...\n"
     ]
    }
   ],
   "source": [
    "opt = get_options(f\"{lib_path}/configs/bert_vggish.py\")\n",
    "logging.info(\"Initializing model...\")\n",
    "# Model\n",
    "try:\n",
    "    network = getattr(networks, opt.model_type)(\n",
    "        num_classes=opt.num_classes,\n",
    "        num_attention_head=opt.num_attention_head,\n",
    "        dropout=opt.dropout,\n",
    "        text_encoder_type=opt.text_encoder_type,\n",
    "        text_encoder_dim=opt.text_encoder_dim,\n",
    "        text_unfreeze=opt.text_unfreeze,\n",
    "        audio_encoder_type=opt.audio_encoder_type,\n",
    "        audio_encoder_dim=opt.audio_encoder_dim,\n",
    "        audio_unfreeze=opt.audio_unfreeze,\n",
    "        audio_norm_type=opt.audio_norm_type,\n",
    "        fusion_head_output_type=opt.fusion_head_output_type,\n",
    "    )\n",
    "    network.to(device)\n",
    "except AttributeError:\n",
    "    raise NotImplementedError(\"Model {} is not implemented\".format(opt.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MMSERA(\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder): Wav2Vec2Base(\n",
       "    (model): Wav2Vec2Model(\n",
       "      (feature_extractor): FeatureExtractor(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): ConvLayerBlock(\n",
       "            (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "            (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          )\n",
       "          (1-4): 4 x ConvLayerBlock(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          )\n",
       "          (5-6): 2 x ConvLayerBlock(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (encoder): Encoder(\n",
       "        (feature_projection): FeatureProjection(\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "            (conv): ParametrizedConv1d(\n",
       "              768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "              (parametrizations): ModuleDict(\n",
       "                (weight): ParametrizationList(\n",
       "                  (0): _WeightNorm()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x EncoderLayer(\n",
       "              (attention): SelfAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (feed_forward): FeedForward(\n",
       "                (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (audio_encoder_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (audio_self_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (text_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (audio_text_cross_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_audio_cross_attention): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (fusion_linear): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fusion_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (classifier): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:46:03,470 - root - INFO - Initializing checkpoint directory and dataset...\n",
      "2023-11-28 19:46:04,068 - root - INFO - \n",
      "             audio_encoder_dim: 768                                     \n",
      "            audio_encoder_type: wav2vec                                 \n",
      "              audio_max_length: 50                                      \n",
      "               audio_norm_type: layer_norm                              \n",
      "                audio_unfreeze: False                                   \n",
      "                    batch_size: 1                                       \n",
      "                checkpoint_dir: d:\\MMSERA\\notebooks\\checkpoints\\bert_vggish_MMSERA\\20231128-194604\n",
      "                     data_root: D:/bert-based-selfalign/IEMOCAP_wav2vec/IEMOCAP_4_class\n",
      "                       dropout: 0.5                                     \n",
      "                      feat_dim: 2048                                    \n",
      "              focal_loss_alpha: None                                    \n",
      "              focal_loss_gamma: 0.5                                     \n",
      "       fusion_head_output_type: mean                                    \n",
      "                      lambda_c: 1.0                                     \n",
      "                 learning_rate: 0.0001                                  \n",
      "           learning_rate_gamma: 0.1                                     \n",
      "       learning_rate_step_size: 30                                      \n",
      "                     loss_type: CrossEntropyLoss                        \n",
      "                margin_loss_m1: 1.0                                     \n",
      "                margin_loss_m2: 0.5                                     \n",
      "                margin_loss_m3: 0.0                                     \n",
      "             margin_loss_scale: 64.0                                    \n",
      "                   max_to_keep: 1                                       \n",
      "                    model_type: MMSERA                                  \n",
      "                          name: bert_vggish_MMSERA                      \n",
      "            num_attention_head: 8                                       \n",
      "                   num_classes: 4                                       \n",
      "                    num_epochs: 250                                     \n",
      "              optim_attributes: None                                    \n",
      "                        resume: False                                   \n",
      "                   resume_path: D:\\MMSERA/notebooks\\checkpoints/bert_vggish_MMSERA/20231118-192914\\weights\\checkpoint_15_60000.pt\n",
      "               save_all_states: True                                    \n",
      "                 save_best_val: True                                    \n",
      "                     save_freq: 7000                                    \n",
      "              text_encoder_dim: 768                                     \n",
      "             text_encoder_type: bert                                    \n",
      "               text_max_length: 297                                     \n",
      "                 text_unfreeze: False                                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.info(\"Initializing checkpoint directory and dataset...\")\n",
    "if opt.text_encoder_type == \"bert\":\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "elif opt.text_encoder_type == \"roberta\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "else:\n",
    "    raise NotImplementedError(\"Tokenizer {} is not implemented\".format(opt.text_encoder_type))\n",
    "\n",
    "# Preapre the checkpoint directory\n",
    "opt.checkpoint_dir = checkpoint_dir = os.path.join(\n",
    "    os.path.abspath(opt.checkpoint_dir),\n",
    "    opt.name,\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    ")\n",
    "log_dir = os.path.join(checkpoint_dir, \"logs\")\n",
    "weight_dir = os.path.join(checkpoint_dir, \"weights\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(weight_dir, exist_ok=True)\n",
    "opt.save(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "train_ds, test_ds = build_train_test_dataset(\n",
    "    opt.data_root,\n",
    "    opt.batch_size,\n",
    "    tokenizer,\n",
    "    opt.audio_max_length,\n",
    "    text_max_length=opt.text_max_length,\n",
    "    audio_encoder_type=opt.audio_encoder_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:46:06,219 - root - INFO - Initializing trainer...\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Initializing trainer...\")\n",
    "if opt.loss_type == \"FocalLoss\":\n",
    "    criterion = losses.FocalLoss(gamma=opt.focal_loss_gamma, alpha=opt.focal_loss_alpha)\n",
    "    criterion.to(device)\n",
    "else:\n",
    "    try:\n",
    "        criterion = getattr(losses, opt.loss_type)(\n",
    "            feat_dim=opt.feat_dim,\n",
    "            num_classes=opt.num_classes,\n",
    "            lambda_c=opt.lambda_c,\n",
    "        )\n",
    "        criterion.to(device)\n",
    "    except AttributeError:\n",
    "        raise NotImplementedError(\"Loss {} is not implemented\".format(opt.loss_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    network=network,\n",
    "    criterion=criterion,\n",
    "    log_dir=opt.checkpoint_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:46:06,251 - root - INFO - Start training...\n",
      "2023-11-28 19:46:06,736 - root - WARNING - When save_best_val is True, please make sure that you pass the validation data to the trainer.fit() method.\n",
      "                            Otherwise, the best model will not be saved.\n",
      "                            The model will save the lowest validation value if the metric starts with 'loss' and the highest value otherwise.\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Start training...\")\n",
    "# Build optimizer and criterion\n",
    "optimizer = optim.Adam(params=trainer.network.parameters(), lr=opt.learning_rate)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt.learning_rate_step_size, gamma=opt.learning_rate_gamma)\n",
    "\n",
    "ckpt_callback = CheckpointsCallback(\n",
    "    checkpoint_dir=weight_dir,\n",
    "    save_freq=opt.save_freq,\n",
    "    max_to_keep=opt.max_to_keep,\n",
    "    save_best_val=opt.save_best_val,\n",
    "    save_all_states=opt.save_all_states,\n",
    ")\n",
    "trainer.compile(optimizer=optimizer, scheduler=lr_scheduler)\n",
    "if opt.resume:\n",
    "    trainer.load_all_states(opt.resume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/250\n",
      "2023-11-28 19:46:07,144 - Training - INFO - Epoch 0/250\n",
      "loss: 0.9625 acc: 1.0000 : : 3979it [09:57,  6.66it/s]                        \n",
      "Epoch 0 - loss: 1.0931\n",
      "2023-11-28 19:56:04,431 - Training - INFO - Epoch 0 - loss: 1.0931\n",
      "Epoch 0 - acc: 0.5184\n",
      "2023-11-28 19:56:04,434 - Training - INFO - Epoch 0 - acc: 0.5184\n",
      "Performing validation...\n",
      "2023-11-28 19:56:04,440 - Training - INFO - Performing validation...\n",
      "100%|##########| 445/445 [00:45<00:00,  9.86it/s]\n",
      "Validation: loss: 0.9686 acc: 0.6135 \n",
      "2023-11-28 19:56:49,893 - Training - INFO - Validation: loss: 0.9686 acc: 0.6135 \n",
      "Model loss improve from inf to 0.9685949584238984, Saving model...\n",
      "2023-11-28 19:56:49,893 - Training - INFO - Model loss improve from inf to 0.9685949584238984, Saving model...\n",
      "Model acc improve from inf to 0.6134831460674157, Saving model...\n",
      "2023-11-28 19:57:28,140 - Training - INFO - Model acc improve from inf to 0.6134831460674157, Saving model...\n",
      "Epoch 1/250\n",
      "2023-11-28 19:58:05,213 - Training - INFO - Epoch 1/250\n",
      "loss: 0.1524 acc: 1.0000 :  76%|#######5  | 3023/3978 [07:33<02:16,  7.00it/s]Saving model at step 7000\n",
      "2023-11-28 20:05:38,602 - Training - INFO - Saving model at step 7000\n",
      "loss: 1.5088 acc: 0.0000 : : 3979it [10:25,  6.36it/s]                          \n",
      "Epoch 1 - loss: 0.9233\n",
      "2023-11-28 20:08:30,688 - Training - INFO - Epoch 1 - loss: 0.9233\n",
      "Epoch 1 - acc: 0.6197\n",
      "2023-11-28 20:08:30,688 - Training - INFO - Epoch 1 - acc: 0.6197\n",
      "Performing validation...\n",
      "2023-11-28 20:08:30,697 - Training - INFO - Performing validation...\n",
      "100%|##########| 445/445 [00:41<00:00, 10.63it/s]\n",
      "Validation: loss: 0.9370 acc: 0.6292 \n",
      "2023-11-28 20:09:12,706 - Training - INFO - Validation: loss: 0.9370 acc: 0.6292 \n",
      "Model loss improve from 0.9685949584238984 to 0.9369554149300865, Saving model...\n",
      "2023-11-28 20:09:12,706 - Training - INFO - Model loss improve from 0.9685949584238984 to 0.9369554149300865, Saving model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\MMSERA\\notebooks\\train.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MMSERA/notebooks/train.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_ds, opt\u001b[39m.\u001b[39;49mnum_epochs, test_ds, callbacks\u001b[39m=\u001b[39;49m[ckpt_callback])\n",
      "File \u001b[1;32md:\\MMSERA\\src\\utils\\torch\\trainer.py:327\u001b[0m, in \u001b[0;36mTorchTrainer.fit\u001b[1;34m(self, train_data, epochs, eval_data, test_data, callbacks)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch, epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    326\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m     global_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch(global_step, epoch, train_data, eval_data, logger, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler(global_step, epoch)\n\u001b[0;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m test_data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MMSERA\\src\\utils\\torch\\trainer.py:122\u001b[0m, in \u001b[0;36mTorchTrainer.train_epoch\u001b[1;34m(self, step, epoch, train_data, eval_data, logger, callbacks)\u001b[0m\n\u001b[0;32m    120\u001b[0m         eval_logs \u001b[39m=\u001b[39m {key: np\u001b[39m.\u001b[39mmean(value) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m eval_logs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    121\u001b[0m         \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m callbacks:\n\u001b[1;32m--> 122\u001b[0m             callback(\u001b[39mself\u001b[39;49m, step, epoch, eval_logs, isValPhase\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, logger\u001b[39m=\u001b[39;49mlogger)\n\u001b[0;32m    123\u001b[0m \u001b[39mreturn\u001b[39;00m step\n",
      "File \u001b[1;32md:\\MMSERA\\src\\utils\\torch\\callbacks.py:126\u001b[0m, in \u001b[0;36mCheckpointsCallback.__call__\u001b[1;34m(self, trainer, global_step, global_epoch, logs, isValPhase, logger)\u001b[0m\n\u001b[0;32m    124\u001b[0m os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_dir, \u001b[39m\"\u001b[39m\u001b[39mbest_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(k)), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_all_states:\n\u001b[1;32m--> 126\u001b[0m     ckpt_path \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49msave_all_states(\n\u001b[0;32m    127\u001b[0m         os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_dir, \u001b[39m\"\u001b[39;49m\u001b[39mbest_\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(k)), \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     ckpt_path \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39msave(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_dir, \u001b[39m\"\u001b[39m\u001b[39mbest_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(k)), \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\MMSERA\\src\\utils\\torch\\trainer.py:225\u001b[0m, in \u001b[0;36mTorchTrainer.save_all_states\u001b[1;34m(self, path, global_epoch, global_step)\u001b[0m\n\u001b[0;32m    222\u001b[0m     checkpoint[\u001b[39m\"\u001b[39m\u001b[39mstate_lr_scheduler\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m    224\u001b[0m ckpt_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39mcheckpoint_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(global_epoch, global_step))\n\u001b[1;32m--> 225\u001b[0m torch\u001b[39m.\u001b[39;49msave(checkpoint, ckpt_path)\n\u001b[0;32m    226\u001b[0m \u001b[39mreturn\u001b[39;00m ckpt_path\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MMSERA\\.venv\\Lib\\site-packages\\torch\\serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m    852\u001b[0m num_bytes \u001b[39m=\u001b[39m storage\u001b[39m.\u001b[39mnbytes()\n\u001b[1;32m--> 853\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(name, storage\u001b[39m.\u001b[39mdata_ptr(), num_bytes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(train_ds, opt.num_epochs, test_ds, callbacks=[ckpt_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3m-ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
