{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\SER_ICIIT_2024\\notebooks\\confusion_matrix.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SER_ICIIT_2024/notebooks/confusion_matrix.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/SER_ICIIT_2024/notebooks/confusion_matrix.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/SER_ICIIT_2024/notebooks/confusion_matrix.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SER_ICIIT_2024/notebooks/confusion_matrix.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m balanced_accuracy_score, accuracy_score, confusion_matrix\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/SER_ICIIT_2024/notebooks/confusion_matrix.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "lib_path = os.path.abspath(\"\").replace(\"notebooks\", \"src\")\n",
    "sys.path.append(lib_path)\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "from transformers import BertTokenizer\n",
    "from data.dataloader import build_train_test_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from models import networks\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(opt, checkpoint_path, tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    network = getattr(networks, opt.model_type)(\n",
    "                num_classes=opt.num_classes,\n",
    "                num_attention_head=opt.num_attention_head,\n",
    "                dropout=opt.dropout,\n",
    "                text_encoder_type=opt.text_encoder_type,\n",
    "                text_encoder_dim=opt.text_encoder_dim,\n",
    "                text_unfreeze=opt.text_unfreeze,\n",
    "                audio_encoder_type=opt.audio_encoder_type,\n",
    "                audio_encoder_dim=opt.audio_encoder_dim,\n",
    "                audio_unfreeze=opt.audio_unfreeze,\n",
    "                audio_norm_type=opt.audio_norm_type,\n",
    "                fusion_head_output_type=opt.fusion_head_output_type,\n",
    "            )\n",
    "    network.to(device)\n",
    "\n",
    "    # Build dataset\n",
    "    _, test_ds = build_train_test_dataset(\n",
    "        opt.data_root,\n",
    "        opt.batch_size,\n",
    "        tokenizer,\n",
    "        opt.audio_max_length,\n",
    "        text_max_length=opt.text_max_length,\n",
    "        audio_encoder_type=opt.audio_encoder_type,\n",
    "    )\n",
    "    # Load checkpoint with save_all_states = False\n",
    "    # network.load_state_dict(torch.load(checkpoint_path, map_location=torch.device(device)).state_dict())\n",
    "    # Load checkpoint with save_all_states = True\n",
    "    print(checkpoint_path)\n",
    "    network.load_state_dict(torch.load(checkpoint_path, map_location=torch.device(device))[\"state_dict_backbone\"])\n",
    "    network.eval()\n",
    "    network.to(device)\n",
    "\n",
    "    y_actu=[]\n",
    "    y_pred=[]\n",
    "\n",
    "    for every_test_list in tqdm(test_ds):\n",
    "        input_ids, audio, label = every_test_list\n",
    "        input_ids = input_ids.to(device)\n",
    "        audio = audio.to(device)\n",
    "        label = label.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = network(input_ids,audio)[0]\n",
    "            _, preds = torch.max(output, 1)\n",
    "            y_actu.append(label.detach().cpu().numpy()[0])\n",
    "            y_pred.append(preds.detach().cpu().numpy()[0])\n",
    "    print(accuracy_score(y_actu, y_pred))\n",
    "    wa = balanced_accuracy_score(y_actu, y_pred)\n",
    "    ua = accuracy_score(y_actu, y_pred)\n",
    "    print(\"Balanced Accuracy: \", wa)\n",
    "    print(\"Accuracy: \", ua)\n",
    "    cm = confusion_matrix(y_actu, y_pred)\n",
    "    print(cm)\n",
    "    cmn = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])*100\n",
    "\n",
    "    ax = plt.subplots(figsize=(8, 5.5))[1]\n",
    "    sns.heatmap(cmn, cmap='YlOrBr', annot=True, square=True, linecolor='black', linewidths=0.75, ax = ax, fmt = '.2f', annot_kws={'size': 16})\n",
    "    ax.set_xlabel('Predicted', fontsize=18, fontweight='bold')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    ax.set_ylabel('Ground Truth', fontsize=18, fontweight='bold')\n",
    "    ax.yaxis.set_ticklabels([\"Anger\", \"Happiness\", \"Sadness\", \"Neutral\"], fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(opt.name + '.png', format='png', dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs.bert_vggish import Config as roberta_wav2vec2_config\n",
    "checkpoint_path = \"D:/SER_ICIIT_2024/notebooks/checkpoints/bert_vggish_MMSERA/20230927-152624\" #check point\n",
    "opt_path = os.path.join(checkpoint_path,\"opt.log\")\n",
    "with open(opt_path, \"r\") as f:\n",
    "    data = f.read().split(\"\\n\")\n",
    "    # remove all empty strings\n",
    "    data = list(filter(None, data))\n",
    "    # convert to dict\n",
    "    data_dict ={}\n",
    "    for i in range(len(data)):\n",
    "        key, value = data[i].split(\":\")[0].strip(), data[i].split(\":\")[1].strip()\n",
    "        if '.' in value and value.replace('.', '').isdigit():\n",
    "            value = float(value)\n",
    "        elif value.isdigit():\n",
    "            value = int(value)\n",
    "        elif value == 'True':\n",
    "            value = True\n",
    "        elif value == 'False':\n",
    "            value = False\n",
    "        elif value == 'None':\n",
    "            value = None\n",
    "        data_dict[key] = value\n",
    "# Load checkpoint with save_all_states = False\n",
    "ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0_0.pt\")\n",
    "# Load checkpoint with save_all_states = False\n",
    "# ckpt_path = os.path.join(checkpoint_path,\"weights/best_acc/checkpoint_0.pt\")\n",
    "opt = roberta_wav2vec2_config()\n",
    "# Replace the default config with the loaded config\n",
    "for key, value in data_dict.items():\n",
    "    setattr(opt, key, value)\n",
    "    \n",
    "# Set dataset path\n",
    "opt.data_root=\"data/IEMOCAP/\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(opt, ckpt_path, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
